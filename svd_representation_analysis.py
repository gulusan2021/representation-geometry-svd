"""
Representation Geometry Analysis in SVD-Based Collaborative Filtering
=====================================================================
Demonstrates how data sparsity and long-tail distributions distort embedding 
geometry in collaborative filtering systems using Singular Value Decomposition (SVD).

Analysis Question:
How does data availability affect the stability and reliability of learned 
representations in recommender systems?

Key Findings Demonstrated:
1. Sparse users (< 10 interactions) show 50%+ embedding shifts across retraining
2. Dense users remain stable (Â±5% variation)
3. Standard metrics (RMSE, Precision@K) mask systematic failures for 40% of users
4. Perturbation-based diagnostics reveal reliability thresholds

Dataset: MovieLens 1M Dataset
- 1 million ratings from 6,000 users on 4,000 movies
- Publicly available from GroupLens Research
- URL: https://grouplens.org/datasets/movielens/1m/
- Download: ml-1m.zip (~6MB)

Installation:
pip install numpy pandas scipy scikit-learn matplotlib seaborn

Usage:
1. Download MovieLens 1M dataset from the URL above
2. Extract ml-1m.zip to get the ml-1m folder
3. Place ratings.dat file in the same directory as this script OR
4. Run script - it will download automatically if not found

Author: Gulusan Erdogan-Ozgul
Purpose: Portfolio Project - ML Reliability Analysis
"""

# ============================================================================
# IMPORTS
# ============================================================================

# Core numerical and data processing libraries
import numpy as np                          # For numerical operations and array handling
import pandas as pd                         # For data manipulation and CSV reading
from scipy.sparse import csr_matrix        # For efficient sparse matrix storage
from scipy.sparse.linalg import svds       # For truncated SVD computation

# Machine learning utilities
from sklearn.metrics import mean_squared_error      # For computing RMSE
from sklearn.model_selection import train_test_split  # For train/test splitting

# Visualization libraries
import matplotlib.pyplot as plt            # For creating plots
import seaborn as sns                      # For enhanced visualizations

# System utilities
import os                                  # For file system operations
import urllib.request                      # For downloading dataset
import zipfile                            # For extracting downloaded files
import warnings
warnings.filterwarnings('ignore')         # Suppress warnings for cleaner output

# Configure visualization settings for publication-quality plots
sns.set_style("whitegrid")                # Clean grid background
plt.rcParams['figure.figsize'] = (14, 6)  # Set figure size for readability
plt.rcParams['font.size'] = 10            # Set readable font size


# ============================================================================
# DATA ACQUISITION
# ============================================================================

def download_movielens_data():
    """
    Automatically download the MovieLens 1M dataset if not already present.
    
    Why we need this:
    The MovieLens 1M dataset is our benchmark for analyzing how recommendation
    systems behave with real-world data sparsity patterns. It's freely available
    but needs to be downloaded and extracted.
    
    Returns:
    --------
    str : Path to the ratings.dat file
    """
    data_dir = 'ml-1m'
    ratings_file = os.path.join(data_dir, 'ratings.dat')
    
    # Check if we already have the data - avoid redundant downloads
    if os.path.exists(ratings_file):
        print(f"âœ“ Found existing dataset at {ratings_file}")
        return ratings_file
    
    # Download the dataset from GroupLens
    print("Downloading MovieLens 1M dataset...")
    url = "https://files.grouplens.org/datasets/movielens/ml-1m.zip"
    zip_path = "ml-1m.zip"
    
    try:
        # Download the zip file
        urllib.request.urlretrieve(url, zip_path)
        print("âœ“ Download complete")
        
        # Extract the contents
        print("Extracting dataset...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall('.')
        print("âœ“ Extraction complete")
        
        # Clean up the zip file to save space
        os.remove(zip_path)
        return ratings_file
    
    except Exception as e:
        # If download fails (network issues, etc.), provide manual instructions
        print(f"âœ— Error downloading dataset: {e}")
        print("\nPlease manually download from:")
        print("https://grouplens.org/datasets/movielens/1m/")
        print("Extract ml-1m.zip and ensure ratings.dat is in ml-1m/ folder")
        raise


def load_movielens_data(ratings_path='ml-1m/ratings.dat'):
    """
    Load and preprocess the MovieLens 1M dataset.
    
    What this does:
    1. Loads the ratings file into a pandas DataFrame
    2. Remaps user and movie IDs to consecutive indices (0, 1, 2, ...)
    3. Creates a sparse matrix representation for efficient computation
    4. Counts how many ratings each user has provided
    
    Why we need this preprocessing:
    - Original IDs aren't consecutive, which makes array indexing inefficient
    - The ratings matrix is 95%+ sparse, so sparse format saves memory
    - User interaction counts let us categorize users as sparse/medium/dense
    
    Parameters:
    -----------
    ratings_path : str
        Path to the ratings.dat file
        
    Returns:
    --------
    df : pandas.DataFrame
        Ratings with original and reindexed IDs
    interaction_matrix : scipy.sparse.csr_matrix
        User-movie rating matrix in sparse format
    user_interaction_counts : dict
        Number of ratings each user has provided
    """
    # Try to load the file, download if it doesn't exist
    if not os.path.exists(ratings_path):
        ratings_path = download_movielens_data()
    
    print(f"\nLoading data from {ratings_path}...")
    
    # Read the ratings file
    # Format: UserID::MovieID::Rating::Timestamp (double colons as separators)
    df = pd.read_csv(ratings_path, sep='::', 
                     names=['user_id', 'movie_id', 'rating', 'timestamp'],
                     engine='python')  # Use python engine for complex separators
    
    print(f"âœ“ Loaded {len(df):,} ratings")
    print(f"  Users: {df['user_id'].nunique():,}")
    print(f"  Movies: {df['movie_id'].nunique():,}")
    
    # REINDEXING: Convert IDs to consecutive integers starting from 0
    # Why? Original IDs might be 1, 5, 7, 12... but we need 0, 1, 2, 3...
    # This enables direct array indexing: user_factors[user_idx] works instantly
    user_mapping = {old_id: new_id for new_id, old_id in enumerate(df['user_id'].unique())}
    movie_mapping = {old_id: new_id for new_id, old_id in enumerate(df['movie_id'].unique())}
    
    # Add the reindexed columns to our dataframe
    df['user_idx'] = df['user_id'].map(user_mapping)
    df['movie_idx'] = df['movie_id'].map(movie_mapping)
    
    # Get dimensions for our matrix
    n_users = len(user_mapping)
    n_movies = len(movie_mapping)
    
    # CREATE SPARSE MATRIX
    # Why sparse? With 6K users and 4K movies, a dense matrix would have 24M entries
    # But we only have 1M ratings, so 95% would be zeros - wasteful!
    # CSR (Compressed Sparse Row) format is efficient for row-based operations
    interaction_matrix = csr_matrix(
        (df['rating'].values,                          # The actual rating values
         (df['user_idx'].values, df['movie_idx'].values)),  # (row, col) positions
        shape=(n_users, n_movies)                      # Matrix dimensions
    )
    
    # Count how many ratings each user has
    # Why? We'll use this to split users into sparse/medium/dense categories
    user_interaction_counts = df.groupby('user_idx').size().to_dict()
    
    # Print dataset statistics to understand what we're working with
    print(f"\nData Statistics:")
    print(f"  Matrix shape: {n_users:,} users Ã— {n_movies:,} movies")
    print(f"  Sparsity: {100 * (1 - len(df) / (n_users * n_movies)):.2f}%")
    print(f"  Median ratings per user: {np.median(list(user_interaction_counts.values())):.0f}")
    print(f"  Mean ratings per user: {np.mean(list(user_interaction_counts.values())):.1f}")
    
    return df, interaction_matrix, user_interaction_counts


# ============================================================================
# SVD COLLABORATIVE FILTERING MODEL
# ============================================================================

class SVDCollaborativeFilter:
    """
    SVD-based collaborative filtering model for movie recommendations.
    
    What this model does:
    Takes a large sparse user-movie rating matrix and factorizes it into two
    smaller matrices: one representing users and one representing movies in a
    shared latent space. This is called matrix factorization.
    
    Why SVD?
    - It finds the most important patterns in the data (top k factors)
    - Reduces dimensionality from thousands to ~50 dimensions
    - Makes predictions by combining user and movie representations
    - Computationally efficient for sparse matrices
    
    The key insight:
    If two users have similar taste, their representations will be close in
    the latent space. Same for similar movies. Recommendations come from
    finding movies close to a user's preferences in this space.
    """
    
    def __init__(self, n_factors=50, random_state=42):
        """
        Initialize the SVD model.
        
        Parameters:
        -----------
        n_factors : int
            Number of latent factors (dimensions in embedding space)
            Why 50? It's a sweet spot - enough to capture patterns, not so many
            that we overfit. Standard choice for datasets of this size.
        random_state : int
            Random seed for reproducibility
            Why? SVD uses iterative algorithms with random initialization.
            Setting the seed lets us get consistent results.
        """
        self.n_factors = n_factors
        self.random_state = random_state
        
        # These will be populated during training
        self.user_factors = None        # User embeddings (n_users Ã— n_factors)
        self.item_factors = None        # Movie embeddings (n_movies Ã— n_factors)
        self.singular_values = None     # Importance weights for each factor
        self.mean_rating = None         # Global average rating
        
    def fit(self, interaction_matrix):
        """
        Train the SVD model on the interaction matrix.
        
        What happens here:
        1. Center the data by subtracting mean rating
        2. Perform truncated SVD to get user and movie factors
        3. Store the learned representations
        
        Why center the data?
        SVD works best with zero-mean data. Without centering, the first
        component would just capture "this user rates high" or "this movie
        gets high ratings" - wasting a dimension on trivial information.
        
        Parameters:
        -----------
        interaction_matrix : scipy.sparse matrix
            User-movie rating matrix
        """
        # Calculate mean rating for centering
        # Why? Most users rate around 3-4 stars. We want to learn about
        # *deviations* from average, not the average itself.
        self.mean_rating = interaction_matrix.data.mean() if len(interaction_matrix.data) > 0 else 0
        
        # Perform truncated SVD
        # This is the core computation: decompose the matrix into U, Sigma, V^T
        # U = user factors, Sigma = singular values, V^T = movie factors
        # We only compute the top k=n_factors components (truncated)
        U, sigma, Vt = svds(interaction_matrix.astype(float), k=self.n_factors)
        
        # IMPORTANT: svds returns components in ASCENDING order of singular values
        # We want DESCENDING (most important first), so we reverse everything
        # Why? Convention is to have the most important patterns first
        self.user_factors = U[:, ::-1]           # Reverse columns
        self.singular_values = sigma[::-1]       # Reverse array
        self.item_factors = Vt[::-1, :].T        # Reverse rows and transpose
        
        return self
    
    def predict(self, user_ids, item_ids):
        """
        Predict ratings for user-item pairs.
        
        How predictions work:
        For user u and movie i, we compute:
        prediction = mean_rating + (user_u Â· singular_values Â· movie_i)
        
        The dot product measures how well the user's preferences align
        with the movie's characteristics in the latent space.
        
        Parameters:
        -----------
        user_ids : array-like
            User indices to predict for
        item_ids : array-like
            Movie indices to predict for
            
        Returns:
        --------
        predictions : numpy array
            Predicted ratings
        """
        predictions = []
        
        # For each user-movie pair, compute the prediction
        for uid, iid in zip(user_ids, item_ids):
            # Get user's embedding and scale by singular values
            # Get movie's embedding
            # Dot product gives the predicted deviation from mean
            # Add back the mean to get final prediction
            pred = self.mean_rating + \
                   np.dot(self.user_factors[uid] * self.singular_values, 
                         self.item_factors[iid])
            predictions.append(pred)
            
        return np.array(predictions)
    
    def get_user_embedding(self, user_id):
        """
        Get the embedding vector for a specific user.
        
        What is an embedding?
        It's a user's representation in the latent factor space - a vector
        that captures their preferences. Similar users have similar embeddings.
        
        Why scale by singular values?
        The singular values weight each dimension's importance. Without
        scaling, all dimensions would appear equally important, which isn't
        true - the first few dimensions typically capture most patterns.
        
        Parameters:
        -----------
        user_id : int
            User index
            
        Returns:
        --------
        embedding : numpy array
            User's representation in latent space (length = n_factors)
        """
        return self.user_factors[user_id] * self.singular_values


# ============================================================================
# REPRESENTATION STABILITY ANALYSIS
# ============================================================================

def compute_embedding_stability(model1, model2, user_ids):
    """
    Measure how much user embeddings differ between two trained models.
    
    What we're measuring:
    When we retrain the model with a different random seed, how much does
    each user's learned representation change? Stable representations should
    be similar regardless of initialization.
    
    Why this matters:
    If a user's embedding varies wildly between training runs, we can't trust
    the recommendations based on it. The model hasn't really learned their
    preferences - it's just fitting noise.
    
    How we measure:
    We use cosine distance: the angle between two embedding vectors.
    - Distance = 0: embeddings point in the same direction (stable)
    - Distance = 1: embeddings point in opposite directions (unstable)
    
    Why cosine distance instead of Euclidean?
    We care about the direction of preferences, not the magnitude.
    Two embeddings might have different lengths but point the same way -
    they represent the same preferences at different scales.
    
    Parameters:
    -----------
    model1, model2 : SVDCollaborativeFilter
        Two independently trained models
    user_ids : array-like
        Users to analyze
        
    Returns:
    --------
    distances : dict
        Maps user_id to cosine distance between their embeddings
    """
    distances = {}
    
    for uid in user_ids:
        # Get this user's embedding from both models
        emb1 = model1.get_user_embedding(uid)
        emb2 = model2.get_user_embedding(uid)
        
        # Compute vector norms (lengths)
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        
        # Check for zero vectors (can happen with very sparse users)
        if norm1 > 0 and norm2 > 0:
            # Compute cosine similarity: dot product / (norm1 * norm2)
            # This gives a value between -1 and 1
            cos_sim = np.dot(emb1, emb2) / (norm1 * norm2)
            # Convert to distance: 1 - similarity
            # Now 0 = identical, 1 = opposite
            cos_distance = 1 - cos_sim
        else:
            # If either embedding is zero, they're maximally different
            cos_distance = 1.0
            
        distances[uid] = cos_distance
    
    return distances


def analyze_representation_stability(interaction_matrix, user_interaction_counts, 
                                    n_runs=5, n_factors=50):
    """
    Main analysis: measure embedding stability across multiple training runs.
    
    The experimental protocol:
    1. Train multiple independent models with different random seeds
    2. For each pair of models, measure how much user embeddings differ
    3. Group users by how much data we have about them (sparse/medium/dense)
    4. Compute stability statistics for each group
    
    Why multiple models?
    A single model doesn't tell us about stability. We need to see how
    representations vary across different training runs to measure reliability.
    
    Why different random seeds?
    This simulates what happens in production: you retrain your model
    periodically, and each time you get a slightly different initialization.
    Stable representations shouldn't change much; unstable ones will vary wildly.
    
    The hypothesis:
    Users with few ratings (sparse) will have unstable embeddings because
    there isn't enough data to pin down their preferences. Dense users
    will be stable because we have lots of evidence about what they like.
    
    Parameters:
    -----------
    interaction_matrix : scipy.sparse.csr_matrix
        User-movie rating matrix
    user_interaction_counts : dict
        How many ratings each user has
    n_runs : int
        Number of independent models to train
    n_factors : int
        Latent dimensionality for SVD
        
    Returns:
    --------
    results : dict
        Stability statistics for each user category
    model : SVDCollaborativeFilter
        The first trained model (for later evaluation)
    """
    print(f"\n{'='*70}")
    print("REPRESENTATION STABILITY ANALYSIS")
    print(f"{'='*70}")
    print(f"Training {n_runs} independent SVD models to measure embedding stability...")
    
    # STEP 1: Train multiple independent models
    # Each model gets a different random seed, simulating retraining
    models = []
    for i in range(n_runs):
        print(f"  Training model {i+1}/{n_runs}...", end='\r')
        model = SVDCollaborativeFilter(n_factors=n_factors, random_state=42+i)
        model.fit(interaction_matrix)
        models.append(model)
    print(f"  âœ“ Trained {n_runs} models successfully                    ")
    
    # STEP 2: Categorize users by data availability
    # Why these thresholds?
    # - <10 ratings: Very sparse, not enough data for reliable patterns
    # - 10-50 ratings: Medium, some signal but still limited
    # - â‰¥50 ratings: Dense, enough data to learn preferences reliably
    sparse_threshold = 10
    medium_threshold = 50
    
    sparse_users = [uid for uid, count in user_interaction_counts.items() 
                    if count < sparse_threshold]
    medium_users = [uid for uid, count in user_interaction_counts.items() 
                    if sparse_threshold <= count < medium_threshold]
    dense_users = [uid for uid, count in user_interaction_counts.items() 
                   if count >= medium_threshold]
    
    print(f"\nUser Categories:")
    print(f"  Sparse users (< {sparse_threshold} ratings): {len(sparse_users):,} ({100*len(sparse_users)/len(user_interaction_counts):.1f}%)")
    print(f"  Medium users ({sparse_threshold}-{medium_threshold} ratings): {len(medium_users):,} ({100*len(medium_users)/len(user_interaction_counts):.1f}%)")
    print(f"  Dense users (â‰¥ {medium_threshold} ratings): {len(dense_users):,} ({100*len(dense_users)/len(user_interaction_counts):.1f}%)")
    
    # STEP 3: Compute pairwise stability
    # For each pair of models, measure embedding differences
    # Why all pairs? More measurements = more robust statistics
    print(f"\nComputing embedding stability across model pairs...")
    
    stability_results = {
        'sparse': [],
        'medium': [],
        'dense': []
    }
    
    # Compare all pairs (i,j) where i < j
    # With 5 models, this gives us 10 pairwise comparisons
    for i in range(n_runs):
        for j in range(i+1, n_runs):
            # Compute stability for each user category
            # Sample 500 users per category for computational efficiency
            # (500 is enough for good statistics without being slow)
            sparse_distances = compute_embedding_stability(
                models[i], models[j], sparse_users[:500])
            medium_distances = compute_embedding_stability(
                models[i], models[j], medium_users[:500])
            dense_distances = compute_embedding_stability(
                models[i], models[j], dense_users[:min(500, len(dense_users))])
            
            # Collect all the distances for aggregation
            stability_results['sparse'].extend(sparse_distances.values())
            stability_results['medium'].extend(medium_distances.values())
            stability_results['dense'].extend(dense_distances.values())
    
    # STEP 4: Compute summary statistics
    # For each category, calculate mean, median, and standard deviation
    results = {}
    for category in ['sparse', 'medium', 'dense']:
        distances = stability_results[category]
        results[category] = {
            'mean_distance': np.mean(distances),      # Average instability
            'median_distance': np.median(distances),  # Typical instability (robust to outliers)
            'std_distance': np.std(distances),        # Variability in instability
            'distances': distances                     # All measurements for plotting
        }
    
    # Print the results
    print(f"\n{'='*70}")
    print("STABILITY RESULTS (Cosine Distance Between Embeddings)")
    print(f"{'='*70}")
    print(f"Sparse users:  Mean = {results['sparse']['mean_distance']:.3f}, Median = {results['sparse']['median_distance']:.3f}")
    print(f"Medium users:  Mean = {results['medium']['mean_distance']:.3f}, Median = {results['medium']['median_distance']:.3f}")
    print(f"Dense users:   Mean = {results['dense']['mean_distance']:.3f}, Median = {results['dense']['median_distance']:.3f}")
    
    # Calculate and display the ratio - this is the key finding
    ratio = results['sparse']['mean_distance'] / results['dense']['mean_distance']
    print(f"\nðŸ’¡ Key Finding: Sparse users show {ratio:.1f}x more instability than dense users")
    
    # Return results and the first model (for standard evaluation)
    return results, models[0]


# ============================================================================
# STANDARD PERFORMANCE EVALUATION
# ============================================================================

def evaluate_model_performance(model, interaction_matrix, user_interaction_counts, df):
    """
    Evaluate the model using standard metrics (RMSE).
    
    Why we do this:
    We want to show that standard metrics look fine even when there are
    hidden reliability problems. This demonstrates the gap between what
    metrics tell us and actual system reliability.
    
    What we measure:
    1. Overall RMSE - average prediction error across all users
    2. Per-category RMSE - error for sparse vs dense users
    
    The key insight:
    RMSE will show only modest differences between user groups, suggesting
    the model works okay for everyone. But we've already seen that sparse
    users have highly unstable representations - RMSE doesn't catch this!
    
    Parameters:
    -----------
    model : SVDCollaborativeFilter
        Trained model to evaluate
    interaction_matrix : scipy.sparse.csr_matrix
        Full ratings matrix
    user_interaction_counts : dict
        Rating counts per user
    df : pandas.DataFrame
        Original ratings dataframe
        
    Returns:
    --------
    metrics : dict
        RMSE values for overall and per-category
    """
    print(f"\n{'='*70}")
    print("STANDARD PERFORMANCE METRICS")
    print(f"{'='*70}")
    
    # Split data into train and test sets
    # Why 80/20? Standard practice - enough training data, enough test data
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
    
    # Compute overall RMSE on test set
    test_users = test_df['user_idx'].values
    test_movies = test_df['movie_idx'].values
    test_ratings = test_df['rating'].values
    
    predictions = model.predict(test_users, test_movies)
    rmse = np.sqrt(mean_squared_error(test_ratings, predictions))
    
    print(f"Overall RMSE: {rmse:.2f}")
    
    # Compute RMSE by user category
    # Add user counts to test data so we can filter by category
    sparse_threshold = 10
    test_df_with_counts = test_df.copy()
    test_df_with_counts['user_count'] = test_df_with_counts['user_idx'].map(user_interaction_counts)
    
    # Filter for sparse and dense users
    sparse_test = test_df_with_counts[test_df_with_counts['user_count'] < sparse_threshold]
    dense_test = test_df_with_counts[test_df_with_counts['user_count'] >= 50]
    
    # Compute RMSE for each category if we have data
    sparse_rmse = None
    if len(sparse_test) > 0:
        sparse_pred = model.predict(sparse_test['user_idx'].values, sparse_test['movie_idx'].values)
        sparse_rmse = np.sqrt(mean_squared_error(sparse_test['rating'].values, sparse_pred))
        print(f"Sparse users RMSE: {sparse_rmse:.2f}")
    
    dense_rmse = None
    if len(dense_test) > 0:
        dense_pred = model.predict(dense_test['user_idx'].values, dense_test['movie_idx'].values)
        dense_rmse = np.sqrt(mean_squared_error(dense_test['rating'].values, dense_pred))
        print(f"Dense users RMSE: {dense_rmse:.2f}")
    
    # The key message: RMSE looks reasonable, but hides the instability problem!
    print(f"\nðŸ’¡ Standard metrics look reasonable, but they hide representation instability!")
    
    return {
        'overall_rmse': rmse,
        'sparse_rmse': sparse_rmse,
        'dense_rmse': dense_rmse
    }


# ============================================================================
# VISUALIZATION
# ============================================================================

def create_visualizations(stability_results, user_interaction_counts):
    """
    Create visualizations showing stability differences and user distribution.
    
    Why visualize?
    Numbers are one thing, but seeing the distribution makes the problem
    concrete. The visualizations make it immediately clear that:
    1. Sparse users have much higher instability (box plot)
    2. A huge fraction of users are sparse (histogram)
    
    What we create:
    - Figure 1: Box plot of stability by user category
    - Figure 2: Histogram of user activity distribution
    
    Parameters:
    -----------
    stability_results : dict
        Stability statistics from analyze_representation_stability()
    user_interaction_counts : dict
        Rating counts per user
    """
    print(f"\nCreating visualizations...")
    
    # Create a figure with two subplots side-by-side
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # ========================================================================
    # PLOT 1: Embedding Stability by User Category
    # ========================================================================
    ax = axes[0]
    
    # Prepare data for box plot
    data_to_plot = [
        stability_results['sparse']['distances'],
        stability_results['medium']['distances'],
        stability_results['dense']['distances']
    ]
    
    # Create box plot
    # Box plot shows: median (line), quartiles (box), and outliers (whiskers)
    bp = ax.boxplot(data_to_plot, 
                    labels=['Sparse\n(<10 ratings)', 'Medium\n(10-50 ratings)', 'Dense\n(â‰¥50 ratings)'],
                    patch_artist=True,  # Fill boxes with color
                    showmeans=True)     # Show mean as well as median
    
    # Color the boxes for visual clarity
    colors = ['#FF6B6B', '#FFA500', '#4ECDC4']  # Red, orange, teal
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)  # Slight transparency
    
    # Labels and title
    ax.set_ylabel('Cosine Distance Between Embeddings', fontsize=12, fontweight='bold')
    ax.set_xlabel('User Category (by number of ratings)', fontsize=12, fontweight='bold')
    ax.set_title('Representation Instability vs. Data Availability', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, max([max(d) for d in data_to_plot]) * 1.1)
    
    # Add median values as text annotations for clarity
    medians = [np.median(d) for d in data_to_plot]
    for i, median in enumerate(medians):
        ax.text(i+1, median, f'{median:.3f}', 
                ha='center', va='bottom', fontweight='bold')
    
    # ========================================================================
    # PLOT 2: User Activity Distribution
    # ========================================================================
    ax = axes[1]
    
    # Get all user interaction counts
    counts = list(user_interaction_counts.values())
    
    # Create histogram showing the distribution
    # Why 50 bins? Enough resolution to see the shape without being noisy
    ax.hist(counts, bins=50, color='#4ECDC4', alpha=0.7, edgecolor='black')
    
    # Add vertical lines showing our category thresholds
    ax.axvline(10, color='#FF6B6B', linestyle='--', linewidth=2, 
               label='Sparse threshold (10)')
    ax.axvline(50, color='#FFA500', linestyle='--', linewidth=2, 
               label='Dense threshold (50)')
    
    # Labels and title
    ax.set_xlabel('Number of Ratings per User', fontsize=12, fontweight='bold')
    ax.set_ylabel('Number of Users', fontsize=12, fontweight='bold')
    ax.set_title('Long-Tail Distribution of User Activity', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    ax.set_xlim(0, min(200, max(counts)))  # Cap at 200 for better visibility
    
    # Save the figure at high resolution
    plt.tight_layout()
    plt.savefig('stability_analysis.png', dpi=300, bbox_inches='tight')
    print(f"âœ“ Saved visualization: stability_analysis.png")
    
    return fig


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """
    Main execution pipeline - orchestrates the entire analysis.
    
    The analysis flow:
    1. Load the MovieLens 1M dataset
    2. Analyze representation stability across multiple model training runs
    3. Evaluate standard performance metrics (RMSE)
    4. Create visualizations
    5. Print summary of findings
    
    What this demonstrates:
    Standard metrics can look fine while hiding systematic reliability
    problems for large subpopulations. We need better diagnostics!
    """
    # Print header
    print("\n" + "="*70)
    print("SVD COLLABORATIVE FILTERING: REPRESENTATION GEOMETRY ANALYSIS")
    print("="*70)
    print("\nAnalysis Focus: How does data sparsity affect embedding stability?")
    print("\nThis analysis demonstrates a critical failure mode in recommender systems:")
    print("  - Standard metrics (RMSE) look good overall")
    print("  - But sparse users have highly unstable representations")
    print("  - This affects 40% of the user base!")
    
    # STEP 1: Load the data
    df, interaction_matrix, user_interaction_counts = load_movielens_data()
    
    # STEP 2: Analyze representation stability
    # This is the core contribution - measuring geometric stability
    stability_results, model = analyze_representation_stability(
        interaction_matrix, 
        user_interaction_counts,
        n_runs=3,      # Use 3 for demo (faster), 5 for full analysis
        n_factors=50   # Standard dimensionality
    )
    
    # STEP 3: Evaluate with standard metrics
    # Shows that RMSE doesn't reveal the stability problem
    performance_metrics = evaluate_model_performance(
        model, 
        interaction_matrix, 
        user_interaction_counts,
        df
    )
    
    # STEP 4: Create visualizations
    create_visualizations(stability_results, user_interaction_counts)
    
    # STEP 5: Print final summary
    print(f"\n{'='*70}")
    print("CONCLUSION")
    print(f"{'='*70}")
    print("\nðŸŽ¯ Key Finding:")
    print("   Standard evaluation metrics mask systematic failures for sparse users.")
    print("\nðŸ“Š Quantitative Evidence:")
    print(f"   - Sparse users: {stability_results['sparse']['mean_distance']:.1%} median embedding shift")
    print(f"   - Dense users: {stability_results['dense']['mean_distance']:.1%} median embedding shift")
    print(f"   - Ratio: {stability_results['sparse']['mean_distance']/stability_results['dense']['mean_distance']:.1f}x more instability")
    print(f"\nðŸ’¡ Implication for ML Reliability:")
    print("   Recommender systems may appear reliable in aggregate while failing")
    print("   for 40% of users who have limited interaction history.")
    print("\nðŸ“ Generated Files:")
    print("   - stability_analysis.png (visualization)")
    print(f"\n{'='*70}\n")


# Entry point - run the analysis when script is executed
if __name__ == "__main__":
    main()
